{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "from functools import partial\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/shared_home/smuckati/miniconda3/envs/relora_sft/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/shared_home/smuckati/miniconda3/envs/relora_sft/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The task is cola\n",
      "unacceptable\n",
      "[29452, 1]\n",
      "Tokenized length is 2\n",
      "acceptable\n",
      "[9961, 1]\n",
      "Tokenized length is 2\n",
      "The task is sst2\n",
      "negative\n",
      "[2841, 1]\n",
      "Tokenized length is 2\n",
      "positive\n",
      "[1465, 1]\n",
      "Tokenized length is 2\n",
      "The task is mrpc\n",
      "not_equivalent\n",
      "[59, 834, 15, 1169, 15592, 1]\n",
      "Tokenized length is 6\n",
      "equivalent\n",
      "[7072, 1]\n",
      "Tokenized length is 2\n",
      "The task is qqp\n",
      "not_duplicate\n",
      "[59, 834, 26, 413, 26221, 1]\n",
      "Tokenized length is 6\n",
      "duplicate\n",
      "[19197, 1]\n",
      "Tokenized length is 2\n",
      "The task is mnli\n",
      "entailment\n",
      "[3, 35, 5756, 297, 1]\n",
      "Tokenized length is 5\n",
      "neutral\n",
      "[7163, 1]\n",
      "Tokenized length is 2\n",
      "contradiction\n",
      "[27252, 1]\n",
      "Tokenized length is 2\n",
      "The task is qnli\n",
      "entailment\n",
      "[3, 35, 5756, 297, 1]\n",
      "Tokenized length is 5\n",
      "not_entailment\n",
      "[59, 834, 35, 5756, 297, 1]\n",
      "Tokenized length is 6\n",
      "The task is rte\n",
      "entailment\n",
      "[3, 35, 5756, 297, 1]\n",
      "Tokenized length is 5\n",
      "not_entailment\n",
      "[59, 834, 35, 5756, 297, 1]\n",
      "Tokenized length is 6\n",
      "The task is wnli\n",
      "not_entailment\n",
      "[59, 834, 35, 5756, 297, 1]\n",
      "Tokenized length is 6\n",
      "entailment\n",
      "[3, 35, 5756, 297, 1]\n",
      "Tokenized length is 5\n"
     ]
    }
   ],
   "source": [
    "# compute the length of the labels\n",
    "tasks = ['cola', 'sst2', 'mrpc', 'qqp', 'mnli', 'qnli', 'rte', 'wnli']\n",
    "tasks_to_labels = {\n",
    "    'cola': ['unacceptable', 'acceptable'],\n",
    "    'sst2': ['negative', 'positive'],\n",
    "    'mrpc': ['not_equivalent', 'equivalent'],\n",
    "    'qqp': ['not_duplicate', 'duplicate'],\n",
    "    # processed differently as this is a regression task\n",
    "    'sts-b': [],\n",
    "    'mnli': ['entailment', 'neutral', 'contradiction'],\n",
    "    'qnli': ['entailment', 'not_entailment'],\n",
    "    'rte': ['entailment', 'not_entailment'],\n",
    "    'wnli': ['not_entailment', 'entailment']\n",
    "}\n",
    "dataset_name = \"glue\"\n",
    "for task in tasks:\n",
    "    print(f\"The task is {task}\")\n",
    "    dataset = load_dataset(dataset_name, task)\n",
    "    labels = tasks_to_labels[task]\n",
    "    for label in labels:\n",
    "        print(label)\n",
    "        label_tokens = tokenizer(label)\n",
    "        print(label_tokens[\"input_ids\"])\n",
    "        input_ids = label_tokens[\"input_ids\"]\n",
    "        print(f\"Tokenized length is {len(input_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue(x, benchmark_name, label_names, feature_names=None, id_key='idx'):\n",
    "    \"\"\"Convert a dataset from glue to text2text examples.\n",
    "\n",
    "    This function uses the feature names from the dataset to unpack examples into\n",
    "    a format amenable for a text2text problem. For example, consider the Quora\n",
    "    Question Pairs (QQP) benchmark, which would suggest\n",
    "    benchmark_name=\"qqp\"\n",
    "    label_names=['not_duplicate', 'duplicate']\n",
    "    For QQP, a typical example might look like\n",
    "    {\n",
    "        \"question1\": \"Why do I easily get bored of my friends?\",\n",
    "        \"question2\": \"Why do I get bored of friends so quickly?\",\n",
    "        \"label\": 1,\n",
    "        \"idx\": 10,\n",
    "    }\n",
    "\n",
    "    This example would be transformed to\n",
    "    {\n",
    "        \"inputs\": (\n",
    "            \"qqp question1: Why do I easily get bored of my friends? question2: \"\n",
    "            \"Why do I get bored of my friends so quickly?\"\n",
    "        ),\n",
    "        \"targets\": \"duplicate\",\n",
    "        \"idx\": 10,\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        x: an example to process.\n",
    "        benchmark_name: the name of the GLUE benchmark for this dataset.\n",
    "        label_names: a list of label names corresponding to class index.\n",
    "        feature_names: an optional ordered list of feature names. If provided,\n",
    "        features will be ordered in this way in the output. If not provided, all\n",
    "        features (except 'idx' and 'label') will be used, sorted by name.\n",
    "        id_key: str, key for id in the dataset. If not provided, 'idx' will be used.\n",
    "        if None, no id will be added to the dataset.\n",
    "\n",
    "    Returns:\n",
    "        A preprocessed example.\n",
    "    \"\"\"\n",
    "    feature_keys = feature_names or sorted(set(x.keys()).difference(['label', 'idx']))\n",
    "    strs_to_join = []\n",
    "    for key in feature_keys:\n",
    "        strs_to_join.append('{}:'.format(key))\n",
    "        strs_to_join.append(x[key])\n",
    "    strs_to_join.insert(0, benchmark_name)\n",
    "    label_name = '<unk>' if x['label'] == -1 else label_names[x['label']]\n",
    "    joined = ' '.join(strs_to_join)\n",
    "\n",
    "    ex = {}\n",
    "    ex['inputs'] = joined\n",
    "    ex['targets'] = label_name\n",
    "\n",
    "    return ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_decoder(examples):\n",
    "        # add a separator between inputs and targets for the model to learn when to predict the targets\n",
    "        inputs = examples['inputs'] + \":\" + examples['targets']\n",
    "        inputs = tokenizer(inputs, return_tensors='pt')\n",
    "\n",
    "        return {'input_ids': len(inputs['input_ids'].squeeze(0))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The task is cola\n",
      "The maximum length inputs for cola is: 54\n",
      "The task is sst2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 67349/67349 [00:13<00:00, 4930.74 examples/s]\n",
      "Map: 100%|██████████| 872/872 [00:00<00:00, 3944.52 examples/s]\n",
      "Map: 100%|██████████| 1821/1821 [00:00<00:00, 4261.65 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for sst2 is: 93\n",
      "The task is mrpc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3668/3668 [00:01<00:00, 2895.26 examples/s]\n",
      "Map: 100%|██████████| 408/408 [00:00<00:00, 2861.30 examples/s]\n",
      "Map: 100%|██████████| 1725/1725 [00:00<00:00, 2879.94 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for mrpc is: 147\n",
      "The task is qqp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 363846/363846 [00:20<00:00, 17531.13 examples/s]\n",
      "Map: 100%|██████████| 40430/40430 [00:02<00:00, 18143.90 examples/s]\n",
      "Map: 100%|██████████| 390965/390965 [00:21<00:00, 18260.71 examples/s]\n",
      "Map: 100%|██████████| 363846/363846 [01:36<00:00, 3758.73 examples/s]\n",
      "Map: 100%|██████████| 40430/40430 [00:10<00:00, 3816.95 examples/s]\n",
      "Map: 100%|██████████| 390965/390965 [01:41<00:00, 3849.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for qqp is: 361\n",
      "The task is mnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 392702/392702 [00:21<00:00, 18026.81 examples/s]\n",
      "Map: 100%|██████████| 9815/9815 [00:00<00:00, 16253.02 examples/s]\n",
      "Map: 100%|██████████| 9832/9832 [00:00<00:00, 17553.48 examples/s]\n",
      "Map: 100%|██████████| 9796/9796 [00:00<00:00, 17195.07 examples/s]\n",
      "Map: 100%|██████████| 9847/9847 [00:00<00:00, 17857.93 examples/s]\n",
      "Map:  43%|████▎     | 170046/392702 [00:51<01:05, 3411.37 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 392702/392702 [01:57<00:00, 3332.28 examples/s]\n",
      "Map: 100%|██████████| 9815/9815 [00:02<00:00, 3439.13 examples/s]\n",
      "Map: 100%|██████████| 9832/9832 [00:02<00:00, 3357.31 examples/s]\n",
      "Map: 100%|██████████| 9796/9796 [00:02<00:00, 3477.49 examples/s]\n",
      "Map: 100%|██████████| 9847/9847 [00:02<00:00, 3416.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for mnli is: 542\n",
      "The task is qnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 104743/104743 [00:05<00:00, 17736.19 examples/s]\n",
      "Map: 100%|██████████| 5463/5463 [00:00<00:00, 16099.65 examples/s]\n",
      "Map: 100%|██████████| 5463/5463 [00:00<00:00, 17406.87 examples/s]\n",
      "Map: 100%|██████████| 104743/104743 [00:34<00:00, 3025.53 examples/s]\n",
      "Map: 100%|██████████| 5463/5463 [00:01<00:00, 2948.05 examples/s]\n",
      "Map: 100%|██████████| 5463/5463 [00:01<00:00, 2992.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for qnli is: 669\n",
      "The task is rte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2490/2490 [00:00<00:00, 14698.77 examples/s]\n",
      "Map: 100%|██████████| 277/277 [00:00<00:00, 10543.33 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 16932.07 examples/s]\n",
      "Map: 100%|██████████| 2490/2490 [00:01<00:00, 2380.24 examples/s]\n",
      "Map: 100%|██████████| 277/277 [00:00<00:00, 2422.97 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:01<00:00, 2596.41 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for rte is: 323\n",
      "The task is wnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 635/635 [00:00<00:00, 9761.63 examples/s]\n",
      "Map: 100%|██████████| 71/71 [00:00<00:00, 6362.20 examples/s]\n",
      "Map: 100%|██████████| 146/146 [00:00<00:00, 11268.99 examples/s]\n",
      "Map: 100%|██████████| 635/635 [00:00<00:00, 3259.52 examples/s]\n",
      "Map: 100%|██████████| 71/71 [00:00<00:00, 1845.73 examples/s]\n",
      "Map: 100%|██████████| 146/146 [00:00<00:00, 2352.63 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for wnli is: 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for task in tasks:\n",
    "    print(f\"The task is {task}\")\n",
    "    dataset = load_dataset(dataset_name, task)  \n",
    "    glue_partial = partial(glue, benchmark_name=task, label_names=tasks_to_labels[task])\n",
    "    column_names = dataset['train'].column_names\n",
    "    dataset = dataset.map(glue_partial, remove_columns=column_names)\n",
    "    old_columns = dataset['train'].column_names\n",
    "    tokenized_dataset = dataset.map(preprocess_function_decoder, remove_columns=old_columns)\n",
    "    len_ids = max([tokenized_dataset[\"train\"][token_ids][\"input_ids\"] for token_ids in range(0, len(tokenized_dataset[\"train\"]))])\n",
    "    print(f\"The maximum length inputs for {task} is: {len_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relora_sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
