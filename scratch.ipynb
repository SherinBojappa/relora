{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import random\n",
    "from functools import partial\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/shared_home/smuckati/miniconda3/envs/relora_sft/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the length of the labels\n",
    "tasks = ['cola', 'sst2', 'mrpc', 'qqp', 'mnli', 'qnli', 'rte', 'wnli']\n",
    "tasks_to_labels = {\n",
    "    'cola': ['unacceptable', 'acceptable'],\n",
    "    'sst2': ['negative', 'positive'],\n",
    "    'mrpc': ['not_equivalent', 'equivalent'],\n",
    "    'qqp': ['not_duplicate', 'duplicate'],\n",
    "    # processed differently as this is a regression task\n",
    "    'sts-b': [],\n",
    "    'mnli': ['entailment', 'neutral', 'contradiction'],\n",
    "    'qnli': ['entailment', 'not_entailment'],\n",
    "    'rte': ['entailment', 'not_entailment'],\n",
    "    'wnli': ['not_entailment', 'entailment']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The task is cola\n",
      "unacceptable\n",
      "[29452, 1]\n",
      "Tokenized length is 2\n",
      "acceptable\n",
      "[9961, 1]\n",
      "Tokenized length is 2\n",
      "The task is sst2\n",
      "negative\n",
      "[2841, 1]\n",
      "Tokenized length is 2\n",
      "positive\n",
      "[1465, 1]\n",
      "Tokenized length is 2\n",
      "The task is mrpc\n",
      "not_equivalent\n",
      "[59, 834, 15, 1169, 15592, 1]\n",
      "Tokenized length is 6\n",
      "equivalent\n",
      "[7072, 1]\n",
      "Tokenized length is 2\n",
      "The task is qqp\n",
      "not_duplicate\n",
      "[59, 834, 26, 413, 26221, 1]\n",
      "Tokenized length is 6\n",
      "duplicate\n",
      "[19197, 1]\n",
      "Tokenized length is 2\n",
      "The task is mnli\n",
      "entailment\n",
      "[3, 35, 5756, 297, 1]\n",
      "Tokenized length is 5\n",
      "neutral\n",
      "[7163, 1]\n",
      "Tokenized length is 2\n",
      "contradiction\n",
      "[27252, 1]\n",
      "Tokenized length is 2\n",
      "The task is qnli\n",
      "entailment\n",
      "[3, 35, 5756, 297, 1]\n",
      "Tokenized length is 5\n",
      "not_entailment\n",
      "[59, 834, 35, 5756, 297, 1]\n",
      "Tokenized length is 6\n",
      "The task is rte\n",
      "entailment\n",
      "[3, 35, 5756, 297, 1]\n",
      "Tokenized length is 5\n",
      "not_entailment\n",
      "[59, 834, 35, 5756, 297, 1]\n",
      "Tokenized length is 6\n",
      "The task is wnli\n",
      "not_entailment\n",
      "[59, 834, 35, 5756, 297, 1]\n",
      "Tokenized length is 6\n",
      "entailment\n",
      "[3, 35, 5756, 297, 1]\n",
      "Tokenized length is 5\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"glue\"\n",
    "for task in tasks:\n",
    "    print(f\"The task is {task}\")\n",
    "    dataset = load_dataset(dataset_name, task)\n",
    "    labels = tasks_to_labels[task]\n",
    "    for label in labels:\n",
    "        print(label)\n",
    "        label_tokens = tokenizer(label)\n",
    "        print(label_tokens[\"input_ids\"])\n",
    "        input_ids = label_tokens[\"input_ids\"]\n",
    "        print(f\"Tokenized length is {len(input_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glue(x, benchmark_name, label_names, feature_names=None, id_key='idx'):\n",
    "    \"\"\"Convert a dataset from glue to text2text examples.\n",
    "\n",
    "    This function uses the feature names from the dataset to unpack examples into\n",
    "    a format amenable for a text2text problem. For example, consider the Quora\n",
    "    Question Pairs (QQP) benchmark, which would suggest\n",
    "    benchmark_name=\"qqp\"\n",
    "    label_names=['not_duplicate', 'duplicate']\n",
    "    For QQP, a typical example might look like\n",
    "    {\n",
    "        \"question1\": \"Why do I easily get bored of my friends?\",\n",
    "        \"question2\": \"Why do I get bored of friends so quickly?\",\n",
    "        \"label\": 1,\n",
    "        \"idx\": 10,\n",
    "    }\n",
    "\n",
    "    This example would be transformed to\n",
    "    {\n",
    "        \"inputs\": (\n",
    "            \"qqp question1: Why do I easily get bored of my friends? question2: \"\n",
    "            \"Why do I get bored of my friends so quickly?\"\n",
    "        ),\n",
    "        \"targets\": \"duplicate\",\n",
    "        \"idx\": 10,\n",
    "    }\n",
    "\n",
    "    Args:\n",
    "        x: an example to process.\n",
    "        benchmark_name: the name of the GLUE benchmark for this dataset.\n",
    "        label_names: a list of label names corresponding to class index.\n",
    "        feature_names: an optional ordered list of feature names. If provided,\n",
    "        features will be ordered in this way in the output. If not provided, all\n",
    "        features (except 'idx' and 'label') will be used, sorted by name.\n",
    "        id_key: str, key for id in the dataset. If not provided, 'idx' will be used.\n",
    "        if None, no id will be added to the dataset.\n",
    "\n",
    "    Returns:\n",
    "        A preprocessed example.\n",
    "    \"\"\"\n",
    "    feature_keys = feature_names or sorted(set(x.keys()).difference(['label', 'idx']))\n",
    "    strs_to_join = []\n",
    "    for key in feature_keys:\n",
    "        strs_to_join.append('{}:'.format(key))\n",
    "        strs_to_join.append(x[key])\n",
    "    strs_to_join.insert(0, benchmark_name)\n",
    "    label_name = '<unk>' if x['label'] == -1 else label_names[x['label']]\n",
    "    joined = ' '.join(strs_to_join)\n",
    "\n",
    "    ex = {}\n",
    "    ex['inputs'] = joined\n",
    "    ex['targets'] = label_name\n",
    "\n",
    "    return ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function_decoder(examples):\n",
    "        # add a separator between inputs and targets for the model to learn when to predict the targets\n",
    "        inputs = examples['inputs'] + \":\" + examples['targets']\n",
    "        inputs = tokenizer(inputs, return_tensors='pt')\n",
    "\n",
    "        return {'input_ids': len(inputs['input_ids'].squeeze(0))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The task is cola\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8551/8551 [00:01<00:00, 4907.11 examples/s]\n",
      "Map: 100%|██████████| 1043/1043 [00:00<00:00, 4577.90 examples/s]\n",
      "Map: 100%|██████████| 1063/1063 [00:00<00:00, 4579.87 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for cola is: 54\n",
      "The task is sst2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 67349/67349 [00:13<00:00, 4867.59 examples/s]\n",
      "Map: 100%|██████████| 872/872 [00:00<00:00, 4103.17 examples/s]\n",
      "Map: 100%|██████████| 1821/1821 [00:00<00:00, 4154.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for sst2 is: 93\n",
      "The task is mrpc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3668/3668 [00:01<00:00, 2923.72 examples/s]\n",
      "Map: 100%|██████████| 408/408 [00:00<00:00, 2859.91 examples/s]\n",
      "Map: 100%|██████████| 1725/1725 [00:00<00:00, 2821.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for mrpc is: 147\n",
      "The task is qqp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 363846/363846 [01:37<00:00, 3732.94 examples/s]\n",
      "Map: 100%|██████████| 40430/40430 [00:11<00:00, 3526.55 examples/s]\n",
      "Map: 100%|██████████| 390965/390965 [01:42<00:00, 3815.68 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for qqp is: 361\n",
      "The task is mnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  43%|████▎     | 170095/392702 [00:50<01:05, 3401.53 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 392702/392702 [01:56<00:00, 3367.38 examples/s]\n",
      "Map: 100%|██████████| 9815/9815 [00:03<00:00, 3132.22 examples/s]\n",
      "Map: 100%|██████████| 9832/9832 [00:02<00:00, 3340.66 examples/s]\n",
      "Map: 100%|██████████| 9796/9796 [00:02<00:00, 3455.32 examples/s]\n",
      "Map: 100%|██████████| 9847/9847 [00:02<00:00, 3390.39 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for mnli is: 542\n",
      "The task is qnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 104743/104743 [00:34<00:00, 3013.47 examples/s]\n",
      "Map: 100%|██████████| 5463/5463 [00:02<00:00, 2460.94 examples/s]\n",
      "Map: 100%|██████████| 5463/5463 [00:01<00:00, 2805.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for qnli is: 669\n",
      "The task is rte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2490/2490 [00:01<00:00, 2457.76 examples/s]\n",
      "Map: 100%|██████████| 277/277 [00:00<00:00, 2251.95 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:01<00:00, 2622.58 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for rte is: 323\n",
      "The task is wnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 635/635 [00:00<00:00, 2996.65 examples/s]\n",
      "Map: 100%|██████████| 71/71 [00:00<00:00, 2806.14 examples/s]\n",
      "Map: 100%|██████████| 146/146 [00:00<00:00, 2849.03 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length inputs for wnli is: 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for task in tasks:\n",
    "    print(f\"The task is {task}\")\n",
    "    dataset = load_dataset(dataset_name, task)  \n",
    "    glue_partial = partial(glue, benchmark_name=task, label_names=tasks_to_labels[task])\n",
    "    column_names = dataset['train'].column_names\n",
    "    dataset = dataset.map(glue_partial, remove_columns=column_names)\n",
    "    old_columns = dataset['train'].column_names\n",
    "    tokenized_dataset = dataset.map(preprocess_function_decoder, remove_columns=old_columns)\n",
    "    len_ids = max([tokenized_dataset[\"train\"][token_ids][\"input_ids\"] for token_ids in range(0, len(tokenized_dataset[\"train\"]))])\n",
    "    print(f\"The maximum length inputs for {task} is: {len_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated string is ['prediction</s><pad><pad><pad>not_entailment<pad><pad>']\n",
      "Label index is 1\n"
     ]
    }
   ],
   "source": [
    "output_text = \"prediction</s><pad><pad><pad>not_entailment<pad><pad>\"\n",
    "decoded_labels = \"not_entailment\"\n",
    "\n",
    "generated_strings = output_text.strip().lower().split(\" \")\n",
    "print(f\"Generated string is {generated_strings}\")\n",
    "\n",
    "#decoded_labels = tokenizer.decode(labels[idx], skip_special_tokens=True)\n",
    "label_index = -1\n",
    "# you will always have one decoded_label even for strings like not_entailment\n",
    "if decoded_labels in output_text:\n",
    "    label_index = tasks_to_labels[\"rte\"].index(decoded_labels)\n",
    "    print(f\"Label index is {label_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prompts from flan templates \n",
    "# https://github.com/google-research/FLAN/blob/main/flan/templates.py\n",
    "import templates\n",
    "from preprocessors import glue, stsb, string_to_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prompt for task cola is ('Sentence: \"{sentence}\"\\nWould a linguist rate this sentence to be acceptable linguistically?\\n\\n{options_}', '{answer}')\n",
      "The prompt for task sst2 is ('Review:\\n{sentence}\\nIs this movie review sentence negative or positive?\\n{options_}', '{answer}')\n",
      "The prompt for task glue_mrpc is ('Here are two sentences:\\n{sentence1}\\n{sentence2}\\nDo they have the same meaning?\\n{options_}', '{answer}')\n",
      "The prompt for task glue_qqp is ('{question1}\\n{question2}\\nWould you say that these questions are the same?\\n{options_}', '{answer}')\n",
      "The prompt for task mnli is ('Premise: {premise}\\n\\nHypothesis: {hypothesis}\\n\\nDoes the premise entail the hypothesis?\\n\\n{options_}', '{answer}')\n",
      "The prompt for task qnli is ('Does the sentence \"{sentence}\" answer the question \"{question}\"\\n\\n{options_}', '{answer}')\n",
      "The prompt for task rte is ('{premise}\\n\\nBased on the paragraph above can we conclude that \"{hypothesis}\"?\\n\\n{options_}', '{answer}')\n",
      "The prompt for task wnli is ('If \"{sentence1}\", can we conclude that \"{sentence2}\"\\n{options_}', '{answer}')\n"
     ]
    }
   ],
   "source": [
    "tasks = ['cola', 'sst2', 'glue_mrpc', 'glue_qqp', 'mnli', 'qnli', 'rte', 'wnli']\n",
    "for task in tasks:\n",
    "    # pick the first prompt in the flan template\n",
    "    prompt = templates.PATTERNS[task][0]\n",
    "    print(f\"The prompt for task {task} is {prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"sts-b\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'Nokia, Texas Instruments and other leading makers of mobile phones have formally complained to Brussels that Qualcomm, the US mobile chipmaker, has unfairly used its patents on 3G technologies.', 'sentence2': 'Nokia produces mobile chips.', 'label': 1, 'idx': 1455}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, DatasetDict\n",
    "benchmark_name = 'rte'\n",
    "dataset = load_dataset('glue', benchmark_name)\n",
    "smaller_dataset = DatasetDict()\n",
    "for split in dataset.keys():\n",
    "    subset_size = 10\n",
    "    smaller_dataset[split] = dataset[split].shuffle(seed=42).select(range(subset_size))\n",
    "dataset = smaller_dataset\n",
    "\n",
    "data = smaller_dataset['train'][2]\n",
    "print(f'{data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entailment', 'not_entailment']\n",
      "('sentence1', 'sentence2')\n",
      "The number of labels are: 2\n",
      "Prompt is ('{premise}\\n\\nBased on the paragraph above can we conclude that \"{hypothesis}\"?\\n\\n{options_}', '{answer}')\n",
      "Template filled is ('Nokia, Texas Instruments and other leading makers of mobile phones have formally complained to Brussels that Qualcomm, the US mobile chipmaker, has unfairly used its patents on 3G technologies.\\n\\nBased on the paragraph above can we conclude that \"Nokia produces mobile chips.\"?\\n\\nA) entailment\\nB) not_entailment', 'B')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_names = tasks_to_labels[benchmark_name]\n",
    "print(label_names)\n",
    "feature_keys = task_to_keys[benchmark_name]\n",
    "print(feature_keys)\n",
    "\n",
    "if benchmark_name == 'rte':\n",
    "    premise = data['sentence1']\n",
    "    hypothesis = data['sentence2']\n",
    "else:\n",
    "    # create variables with the same name as the strings - will need it to fill in the template\n",
    "    variables = task_to_keys.get(benchmark_name, ())\n",
    "    locals().update({var: data[var] for var in variables})\n",
    "\n",
    "# find out how many labels are there\n",
    "num_labels = len(tasks_to_labels[benchmark_name])\n",
    "print(f\"The number of labels are: {num_labels}\")\n",
    "if num_labels == 3:\n",
    "    options_ = f'A) {label_names[0]}\\nB){label_names[1]}\\nC){label_names[2]}' # You can modify the options as needed\n",
    "    if data['label'] == 0:\n",
    "        answer = 'A'\n",
    "    elif data['label'] == 1:\n",
    "        answer = 'B'\n",
    "    elif data['label'] == 2:\n",
    "        answer = 'C'\n",
    "elif num_labels == 2:\n",
    "    options_ = f'A) {label_names[0]}\\nB) {label_names[1]}' # You can modify the options as needed\n",
    "    answer = 'A' if data['label'] == 0 else 'B' # Modify according to the label mapping\n",
    "\n",
    "prompt_template = templates.PATTERNS[benchmark_name][0][0] # extract first prompt and since its a tuple extract string\n",
    "print(f'Prompt is {prompt}')\n",
    "filled_prompt = prompt_template.format(\n",
    "    premise=premise,\n",
    "    hypothesis=hypothesis,\n",
    "    sentence=locals().get('sentence', ''),\n",
    "    sentence1=locals().get('sentence1', ''),\n",
    "    sentence2=locals().get('sentence2', ''),\n",
    "    question=locals().get('question', ''),\n",
    "    question1=locals().get('question1', ''),\n",
    "    question2=locals().get('question2', ''),\n",
    "    options_=options_,\n",
    ")\n",
    "\n",
    "template_filled = (filled_prompt, answer)\n",
    "\n",
    "\n",
    "print(f'Template filled is {template_filled}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "relora_sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
